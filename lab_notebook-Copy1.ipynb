{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"Title\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#always run \"source ~/anaconda3/bin/activate root\" before running anaconda-navigator to open this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# LAB NOTEBOOK\n",
    "## PROF. ANDREW C.R. MARTIN\n",
    "## Project: CDR H3 Modelling Quality Prediction\n",
    "### By: Lilian Denzler, Msci Student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Antibody modelling has come a far way in the recent decade. A great achievement has been the development of many sophisticated software programmes, such as \"abYmod\" written by Prof. Martin at UCL. \n",
    "\n",
    "- recent developments in the field\n",
    "- problems i.e. CDRH3 modelling\n",
    "- why modelling is so hard (biological background)\n",
    "- quality assesment methods\n",
    "- how this is different\n",
    "\n",
    "<b>Project aim:</b>\n",
    "The aim is to have by the end of the project: \n",
    "\n",
    "<b>Plan:</b>\n",
    "\n",
    "1) Dataset creation\n",
    "- AbDb pdb data\n",
    "- sequence extraction\n",
    "- antibody modelling dataset\n",
    "- template dataset\n",
    "\n",
    "2) RMS deviation calculations\n",
    "\n",
    "3) Feature set creation\n",
    "- sequence identity of template scores\n",
    "- length\n",
    "- hydrophobicity score\n",
    "-...\n",
    "\n",
    "4) Feature encoding\n",
    "\n",
    "5) ML models\n",
    "\n",
    "6) Evaluation\n",
    "\n",
    "7) stand-alone web-programme\n",
    "\n",
    "\n",
    "\n",
    "Maybe make this notebook reproduce whole project, make inserted directories at start be the directories for all following commands??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "[Title](#Title)\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "[Setup and Organization](#SetupandOrganization)\n",
    "\n",
    "[1. Dataset Generation](#DatasetGeneration)\n",
    "\n",
    "[1.1 Antibody Sequence Parsing](#AntibodySequenceParsing)\n",
    "\n",
    "[1.2 AbYmod Modelling](#AbYmodModelling)\n",
    "\n",
    "[1.3 RMSD Calculation](#RMSDCalculation)\n",
    "\n",
    "[2. Preliminary Model](#PreliminaryModel)\n",
    "\n",
    "[3. Feature Extraction](#FeatureExtraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"SetupandOrganization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Setup and Organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This lab-notebook (written in Jupyter-notebook, running a bash kernel) will include be used to document the workflow of this project. In order to ensure reproducibility of results, bash shell commands will be included in the following and all used employed programs are to be found on github (https://github.com/LilianDenzler/CDR_H3_quality/tree/master)\n",
    "\n",
    "<i>Calender Week 40 & 41</i>\n",
    "\n",
    "After discussing the options for computer setup with Prof. Martin, I decided to opt for running a Linux OS (Ubuntu 20.04.1 LTS) in a Virtual Machine on my Windows-Laptop. I used VirtualBox (https://www.virtualbox.org/) for this and booted the VM from an external hard-drive. This enabled me to use the BASH shell without any compliations, and also gave me additional storage for any large files I didn't want to save on my laptop. \n",
    "\n",
    "Afterwards, I set up Git in order to track changes. I also set up a GitHub account (User: LilianDenzler) and made a new repository titled \"CDR_H3_quality\". Then, I synchronized Git with Github. The directory <i>/git</i> contains all files that are pushed to the repository. Prof. Martin showed me how to log into his personal server remotely, which had abYmod, bioptools and csv2arff installed on it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<i>Calender Week 42</i>\n",
    "\n",
    "Unison was set up to sync files between the remote server and my local client. This is important for backing up data and enables me to work with large datasets, which my laptop does not have sufficient computing power for. The synced data is saved in the <i> /sync_project</i> directory. Files in the <i> /NOSYNC</i> subdirectory will not be synced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"DatasetGeneration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The first step is to produce a dataset of Antibody models, which can then be examined regarding their quality in following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "![alt: Workflow for Dataset creation](Dataset_creation.png \"Organisation of Datasets, Scripts, Documents, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "*Fig 1.* Workflow for Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"AntibodySequenceParsing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 1.1 Antibody Sequence Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<i>Calender Week 42 & 43</i>\n",
    "\n",
    "The Chothia-numbered non-redundant Antibody stucture AbDb-dataset from bioinf.org.uk/abs/abdb was downloaded. The sequence of these Antibodies had to be extracted from these PDB files. To acomplish this, I wrote a short python script (\"seq_parser.py\"), which returned the sequence in the following abYmod-compatible format:\n",
    "\n",
    "L1 D  \n",
    "L2 I                 \n",
    "L3 Q        \n",
    "L4 M                 \n",
    "...    \n",
    "H1 F      \n",
    "H2 M      \n",
    "H3 T      \n",
    "H4 Q     \n",
    "...     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "python3 seq_parser.py ~/sync_project/actual_PDBs_NR ~/sync_project/input_Abymod_seq\n",
    "#argv[1]= direcoty with actual PDBs, argv[2] = directory to keep input sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"AbYmodModelling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 1.2 AbYmod Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "However, some of the antibodies in my newly-made dataset had redundant antibodies in the complete, redundant AbDb-dataset which is used by abYmod. I thus downloaded the textfile of PDB codes of redundant structures (also on bioinf.org.uk/abs/abdb). A python script formatted this into a dictionary. This dictionary will be used during the abYmod modelling step: For all input sequences it will be used to check if there are any redundant structures in the dataset, which will then be excluded along with the original PDB structure. This is important, as otherwise the modelling software will have an exact template and thus achieve 100% accuracy in modelling.  \n",
    "\n",
    "The get_abYmod2.py passes a command to abYmod and saves the modelled PDB file under \"<PDB_ID>.pdb.model\". During the modelling process the redundant structures are excluded as templates. The template files abYmod creates during modelling (these contain,among other things, the PDB codes of the best structural template used for modelling for the CDRH3 loop and are thus of importance)are saved in a separate subdirectory as \"<PDB_ID>.tpl\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 14572\n",
      "[1]+  Exit 1                  nohup python3 get_abYmod2.py /home/lilian/sync_project/Redundant_PDBs.txt /home/lilian/sync_project/NOSYNC/input_Abymod_seq/ /home/lilian/sync_project/NOSYNC/abYmod_structures/ &> output_abymod.log\n"
     ]
    }
   ],
   "source": [
    "nohup python3 get_abYmod2.py /home/lilian/sync_project/Redundant_PDBs.txt /home/lilian/sync_project/NOSYNC/input_Abymod_seq/ /home/lilian/sync_project/NOSYNC/abYmod_structures/ &>output_abymod.log &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#argv[1]=Redundant_PDB file; argv[2]=directory of .seq files; argv[3]=directory to save model.pdb files;\n",
    "#[1] 2683019 Process ID (in case you need to kill process)\n",
    "#use \"nohup\" so command can continue running in the background after you log out\n",
    "#use \"&\" so it runs in background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"RMSDCalculation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 1.3 RMSD Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "In order the make a judgement on the accuracy of the generated model, we will not calculate the Root Mean Square Deviation values (actal structures vs. modelled structures). \n",
    "\n",
    "ProFit (http://www.bioinf.org.uk/software/profit/) is used to make these calculations. The atom-to-atom RMS as well as the C-alpha atom to C-alpha atom of each residue is calculated. \n",
    "The C-alpha-atom values give a better indication of the validity of the modelled overall backbone structure. The atom values give additional information on how well the side-chains are modelled.\n",
    "\n",
    "To determine whether the loop conformation itself is modelled incorrectly, a local calculation is performed. Here, the CDRH3 loops of the actual structure and the model are fitted onto each other.\n",
    "To determine whether the loop positioning is modelled incorrectly, a global calculation is performed. The entire structure is fitted for this. \n",
    "\n",
    "\n",
    "MAKE PYMOL GRAPH TO VISUALIZE WHAT THE FITTING MEANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "nohup python3 save_RMS ~/sync_project/programs/RMS_calc ~/sync_project/actual_PDBs_NR ~/sync_project/NOSYNC/abYmod_structures &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We now have a single .csv file with all RMSD values for each antibody in our database. Additionally, we have a file containing the \"by_residue\" RMSD values for each structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Moving forward, not all RMSD parameters will be used. To ensure that all RMSD values are equally informative, we check for correlation of the different RMSD-values. The tight linear positive correlation supports the assumption, that any of the RMSD value sets may be used for the preliminary machine learning model. I chose the global atom-to-atom RMSD set, as it best represents the total error made. Here, all information on absolute loop positioning with respect to the whole structure, loop conformation and side-chain positioning converges. It therefore seems the most useful figure for preliminary analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "![alt: Workflow for Dataset creation](RMSD_graph.png \"Organisation of Datasets, Scripts, Documents, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "*Fig.2* This graph (plotted using the script graphs.py) shows the tight correlation of the differnet RMSD values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"PreliminaryModel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 2. Preliminary Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Before extracting many different features that may be useful for making an accuracte prediction of CDRH3 loop modelling quality we want to build a preliminary model. We will only use the loop length feature sequence identity of the template used to model the loop for this model.\n",
    "\n",
    "It will give us a general idea of how accurate such a very basic model can be and how many features may need to be added. It is also a good opportunity to test different ML models. I have decided to first build a neural network. \n",
    "(INSERT REASONS WHY)\n",
    "\n",
    "Before building the model, we first need to generate a csv file containing the following information:\n",
    "- Sequence identity (protein sequence of template loop vs of target sequence)\n",
    "- Sequence similarity\n",
    "- loop length\n",
    "- PDB Code\n",
    "\n",
    "The sequence identity and similarity is extracted from the abYmod log files. the sequences of the template and target loop are also extracted for later use. We make a csv file of sequence identity, similarity and PDB code. (We include the PDB code to ensure the feature values belong to the same modelled PDB structure.) \n",
    "First, we move the log files to a separate directory and then run the script for generating the csv file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "ssh shed\n",
    "cd /sync_project\n",
    "mkdir abYmod_log\n",
    "cd ~/sync_project/abYmod_structures\n",
    "mv *.log ~/sync_project/abYmod_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The loop length is then extracted from the input sequence files and aso saved as a csv file together with the PDB codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "python3 -c 'import feature_extraction; feature_extraction.get_loop_length(~/sync_project/Feature, ~/sync_project/input_Abymod_seq)'\n",
    "#arg 1= Feature directory, arg 2= input sequence directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We will then merge the two csv files we have just made with each other and with the RMSD value file.\n",
    "on the 'ID' column. This ensures that each feature is matches to it's correct structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "python3 -c 'import feature_extraction; feature_extraction.merge_csv(~/sync_project/Feature/RMS_feature_csv.csv, ~/sync_project/Feature/length_feature_csv.csv, length_RMS, ~/sync_project/Feature/)'\n",
    "python3 -c 'import feature_extraction; feature_extraction.merge_csv(~/sync_project/Feature/length_RMS.csv, ~/sync_project/Feature/seq_id.csv, length_RMS_seqId, ~/sync_project/Feature/)'\n",
    "#arg 1= file a, arg 2= file b, arg 3= new file name, arg 4= Feature directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We then convert this csv file into an .arff file, which is the input file type for weka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "csv2arff -inputs=identity,similarity,length global_AA ~/sync_project/Feature/length_RMS_seqId.csv > ~/sync_project/Feature/length_RMS_seqId.arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "<a id=\"FeatureExtraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This input file can now be used in weka. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Feature extraction is the most important part of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": false,
    "height": 0
   },
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
